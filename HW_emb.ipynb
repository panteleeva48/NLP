{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание\n",
    "Возьмите данные соревнования по определению перефразирования - http://paraphraser.ru/download/get?file_id=1 \n",
    "Если не работает, то отсюда - https://drive.google.com/file/d/1rxm3K-0e1v4NYDzKq1Zi3FsXvh1yDakz/view?usp=sharing\n",
    "Преобразуйте тексты в векторы в каждой паре 4 методами  - SVD, NMF, Word2Vec, Fastext. Для SVD и NMF сделайте две пары векторов - через TfidfVectorizer и CountVectorizer. Для word2vec сделайте две пары векторов - с взвешиванием по tfidf и без. Для Fastext постройте две модели - без нормализации и с нормализацией, а через каждую модель постройте две пары векторов -  с взвешиванием по tfidf и без. \n",
    "\n",
    "Для обучения этих моделей можете воспользоваться корпусом новостных текстов, с которым мы работали на семинаре. А можете использовать любой другой корпус (сами тексты соревнования использовать не надо).\n",
    "\n",
    "У вас должно получиться 10 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 10 чисел для каждой пары текстов). \n",
    "\n",
    "Постройте обучающую выборку из этих близостей. Обучите любую модель (Логрег, Рандом форест или что-то ещё) на этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру).   \n",
    "\n",
    "С помощью кросс-валидации подберите параметры моделей (количество компонент, размерность в w2v, min_n - в fastext и т.д). Перебирать все параметры и их сочетания не нужно! Можно перебрать несколько значений для какого-то параметра (например, 50, 100, 200 для svd), понять примерное влияние на метрику и переходить к следующему. Цель в этом пункте - найти \n",
    "\n",
    "Оценивание - если вы сделали всё вышеперечисленное - 10 баллов. Каждая ошибка - минус 0.5 балла. \n",
    "\n",
    "\n",
    "Выложите код к себе на гитхаб и вставьте ссылку в поле ниже (в тетрадке сохраните показатели метрик)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные, которые мы преобразуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
       "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>право полицейский проникновение жилища решить ...</td>\n",
       "      <td>правило внесудебный проникновение полицейский ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>президент египет ввести чрезвычайный положение...</td>\n",
       "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             text_1  \\\n",
       "0     0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1     0  Право полицейских на проникновение в жилище ре...   \n",
       "2     0  Президент Египта ввел чрезвычайное положение в...   \n",
       "3    -1  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4     0  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...   \n",
       "1  Правила внесудебного проникновения полицейских...   \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...   \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...   \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский разрешить стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилища решить ...   \n",
       "2  президент египет ввести чрезвычайный положение...   \n",
       "3  вернуться сирия россиянин волновать вопрос тру...   \n",
       "4  москва сирия вернуться 2 самолёт мчс россиянин...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0  полиция мочь разрешить стрелять хулиган травма...  \n",
       "1  правило внесудебный проникновение полицейский ...  \n",
       "2  власть египет угрожать ввести страна чрезвычай...  \n",
       "3      самолёт мчс вывезти россиянин разрушить сирия  \n",
       "4      самолёт мчс вывезти россиянин разрушить сирия  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_for_model = pd.read_csv('news_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Канцлер Германии Ангела Меркель в ходе брифинг...</td>\n",
       "      <td>канцлер германия ангел меркель ход брифинг пре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Российские и белорусские войска успешно заверш...</td>\n",
       "      <td>российский белорусский войско успешно завершит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Дзюба, Шатов и Анюков оказались не нужны «Зени...</td>\n",
       "      <td>дзюба шат анюк оказаться нужный зенит российск...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Испанию без фанатов\\nПожалуй, главной пятнич...</td>\n",
       "      <td>испания фанат пожалуй главный пятничный новост...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Постпред России при ООН Виталий Чуркин, говоря...</td>\n",
       "      <td>постпред россия оон виталий чуркин говорить ве...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Канцлер Германии Ангела Меркель в ходе брифинг...   \n",
       "1  Российские и белорусские войска успешно заверш...   \n",
       "2  Дзюба, Шатов и Анюков оказались не нужны «Зени...   \n",
       "3  В Испанию без фанатов\\nПожалуй, главной пятнич...   \n",
       "4  Постпред России при ООН Виталий Чуркин, говоря...   \n",
       "\n",
       "                                        content_norm  \n",
       "0  канцлер германия ангел меркель ход брифинг пре...  \n",
       "1  российский белорусский войско успешно завершит...  \n",
       "2  дзюба шат анюк оказаться нужный зенит российск...  \n",
       "3  испания фанат пожалуй главный пятничный новост...  \n",
       "4  постпред россия оон виталий чуркин говорить ве...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_model.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_model = data_for_model.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование в векторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "import numpy as np\n",
    "from gensim import matutils\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_result(X_text, y):\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(X_text, y, random_state=1)\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_leaf=10,\n",
    "                             class_weight='balanced')\n",
    "    clf.fit(train_X, train_y)\n",
    "    preds = clf.predict(valid_X)\n",
    "    return 'F1-score: ' + str(metrics.f1_score(valid_y, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_v = CountVectorizer(max_features=5000)\n",
    "c_v = c_v.fit(data_for_model['content_norm'])\n",
    "X_c_v = c_v.transform(data_for_model['content_norm'])\n",
    "\n",
    "tfidf_v = TfidfVectorizer(max_features=5000)\n",
    "tfidf_v = tfidf_v.fit(data_for_model['content_norm'])\n",
    "X_tfidf_v = tfidf_v.transform(data_for_model['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(128)\n",
    "svd.fit(X_c_v)\n",
    "models['svd_c_v'] = svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(128)\n",
    "svd.fit(X_tfidf_v)\n",
    "models['svd_tfidf_v'] = svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(128)\n",
    "nmf.fit(X_c_v)\n",
    "models['nmf_c_v'] = nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(128)\n",
    "nmf.fit(X_tfidf_v)\n",
    "models['nmf_tfidf_v'] = nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nmf_c_v': NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "   n_components=128, random_state=None, shuffle=False, solver='cd',\n",
       "   tol=0.0001, verbose=0),\n",
       " 'nmf_tfidf_v': NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "   n_components=128, random_state=None, shuffle=False, solver='cd',\n",
       "   tol=0.0001, verbose=0),\n",
       " 'svd_c_v': TruncatedSVD(algorithm='randomized', n_components=128, n_iter=5,\n",
       "        random_state=None, tol=0.0),\n",
       " 'svd_tfidf_v': TruncatedSVD(algorithm='randomized', n_components=128, n_iter=5,\n",
       "        random_state=None, tol=0.0)}"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vectors(data, model, vctrzr):\n",
    "    text_1 = data.text_1_norm\n",
    "    text_2 = data.text_2_norm\n",
    "    \n",
    "    text2v_1 = vctrzr.transform(text_1)\n",
    "    text2v_2 = vctrzr.transform(text_2)\n",
    "    \n",
    "    v2v_1 = model.transform(text2v_1)\n",
    "    v2v_2 = model.transform(text2v_2)\n",
    "    return v2v_1, v2v_2, [float(cosine_similarity([x], [y])) for x, y in zip(v2v_1, v2v_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 277 ms, total: 12.1 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svd_c_v_1, svd_c_v_2, cos_svd_c_v = get_vectors(data, models['svd_c_v'], c_v)\n",
    "nmf_c_v_1, nmf_c_v_2, cos_nmf_c_v = get_vectors(data, models['nmf_c_v'], c_v)\n",
    "svd_tfidf_v_1, svd_tfidf_v_2, cos_svd_tfidf = get_vectors(data, models['svd_tfidf_v'], tfidf_v)\n",
    "nmf_tfidf_v_1, nmf_tfidf_v_2, cos_nmf_tfidf = get_vectors(data, models['nmf_tfidf_v'], tfidf_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NMF + CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text = np.concatenate([nmf_c_v_1, nmf_c_v_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.43192616842729853'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NMF + TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text = np.concatenate([nmf_tfidf_v_1, nmf_tfidf_v_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.43626449117140753'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVD + CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text = np.concatenate([svd_c_v_1, svd_c_v_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.4455993288985442'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVD + TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text = np.concatenate([svd_tfidf_v_1, svd_tfidf_v_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.4706772922077586'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_tf(text):\n",
    "    tf_text = Counter(text)\n",
    "    for i in tf_text:\n",
    "        tf_text[i] = tf_text[i]/float(len(text))\n",
    "    return tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_idf(word, corpus):\n",
    "    return math.log10(len(corpus)/sum([1.0 for i in corpus if word in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(corpus):\n",
    "    documents_list = []\n",
    "    for text in tqdm(corpus):\n",
    "        tf_idf_dictionary = {}\n",
    "        computed_tf = compute_tf(text)\n",
    "        for word in computed_tf:\n",
    "            tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, corpus)\n",
    "        documents_list.append(tf_idf_dictionary)\n",
    "    return documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "together = list(data['text_1_norm'].values) + list(data['text_2_norm'].values)\n",
    "corpus = [text.split() for text in together]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0480f501771a42c795e1298585021c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf = compute_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf_text_1, tf_idf_text_2 = tf_idf[:data.shape[0]], tf_idf[data.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim, tfidf=False):\n",
    "    text = text.split()\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            if tfidf:\n",
    "                vectors[i] = v*tfidf[word]\n",
    "            else:\n",
    "                vectors[i] = v*(words[word]/total)\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec([text.split() for text in data_for_model['content_norm']], size=128, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### БЕЗ TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm'].values), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm'].values), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.48849320026426746'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text_w2v, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_w2v = [float(cosine_similarity([x], [y])) for x, y in zip(X_text_1_w2v, X_text_2_w2v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm'].values), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm'].values), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim, tfidf=tf_idf_text_1[i])\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim, tfidf=tf_idf_text_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.48043122317367576'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text_w2v, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_w2v_tfidf = [float(cosine_similarity([x], [y])) for x, y in zip(X_text_1_w2v, X_text_2_w2v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text = gensim.models.FastText([text.split() for text in data_for_model['content_norm']], size=128, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### БЕЗ TFIDF, БЕЗ НОРМАЛИЗАЦИИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "data['text_1_notnorm'] = data['text_1'].apply(tokenize)\n",
    "data['text_2_notnorm'] = data['text_2'].apply(tokenize)\n",
    "\n",
    "X_text_1_ft = np.zeros((len(data['text_1_notnorm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_notnorm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_notnorm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_notnorm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.4721337121674352'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text_ft, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_ft = [float(cosine_similarity([x], [y])) for x, y in zip(X_text_1_ft, X_text_2_ft)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### БЕЗ TFIDF, C НОРМАЛИЗАЦИЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm'].values), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm'].values), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.49046982893835245'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text_ft, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_ft_norm = [float(cosine_similarity([x], [y])) for x, y in zip(X_text_1_ft, X_text_2_ft)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C TFIDF, C НОРМАЛИЗАЦИЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm'].values), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm'].values), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text, dim, tfidf=tf_idf_text_1[i])\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text, dim, tfidf=tf_idf_text_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.47838389271955695'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text_ft, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_ft_norm_tfidf = [float(cosine_similarity([x], [y])) for x, y in zip(X_text_1_ft, X_text_2_ft)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C TFIDF, БЕЗ НОРМАЛИЗАЦИИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "X_text_1_ft = np.zeros((len(data['text_1_notnorm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_notnorm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_notnorm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text, dim, tfidf=tf_idf_text_1[i])\n",
    "    \n",
    "for i, text in enumerate(data['text_2_notnorm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text, dim, tfidf=tf_idf_text_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.4715568184675736'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X_text_ft, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_ft_tfidf = [float(cosine_similarity([x], [y])) for x, y in zip(X_text_1_ft, X_text_2_ft)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "d = {'cos_svd_c_v': cos_svd_c_v, \n",
    "     'cos_nmf_c_v': cos_nmf_c_v, \n",
    "     'cos_svd_tfidf': cos_svd_tfidf, \n",
    "     'cos_nmf_tfidf': cos_nmf_tfidf, \n",
    "     'cos_w2v': cos_w2v, \n",
    "     'cos_w2v_tfidf': cos_w2v_tfidf, \n",
    "     'cos_ft': cos_ft, \n",
    "     'cos_ft_tfidf': cos_ft_tfidf, \n",
    "     'cos_ft_norm_tfidf': cos_ft_norm_tfidf, \n",
    "     'cos_ft_norm': cos_ft_norm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_ft</th>\n",
       "      <th>cos_ft_norm</th>\n",
       "      <th>cos_ft_norm_tfidf</th>\n",
       "      <th>cos_ft_tfidf</th>\n",
       "      <th>cos_nmf_c_v</th>\n",
       "      <th>cos_nmf_tfidf</th>\n",
       "      <th>cos_svd_c_v</th>\n",
       "      <th>cos_svd_tfidf</th>\n",
       "      <th>cos_w2v</th>\n",
       "      <th>cos_w2v_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.776740</td>\n",
       "      <td>0.697593</td>\n",
       "      <td>0.715177</td>\n",
       "      <td>0.644239</td>\n",
       "      <td>0.521702</td>\n",
       "      <td>0.931057</td>\n",
       "      <td>0.137964</td>\n",
       "      <td>0.627718</td>\n",
       "      <td>0.849223</td>\n",
       "      <td>0.849223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.743313</td>\n",
       "      <td>0.772822</td>\n",
       "      <td>0.779357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194543</td>\n",
       "      <td>0.920925</td>\n",
       "      <td>0.395494</td>\n",
       "      <td>0.738407</td>\n",
       "      <td>0.872423</td>\n",
       "      <td>0.872423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.668038</td>\n",
       "      <td>0.802900</td>\n",
       "      <td>0.812663</td>\n",
       "      <td>0.632182</td>\n",
       "      <td>0.007293</td>\n",
       "      <td>0.893944</td>\n",
       "      <td>0.094306</td>\n",
       "      <td>0.709270</td>\n",
       "      <td>0.928296</td>\n",
       "      <td>0.928296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.683037</td>\n",
       "      <td>0.595254</td>\n",
       "      <td>0.581431</td>\n",
       "      <td>-0.057154</td>\n",
       "      <td>0.511020</td>\n",
       "      <td>0.897832</td>\n",
       "      <td>0.575513</td>\n",
       "      <td>0.689159</td>\n",
       "      <td>0.644343</td>\n",
       "      <td>0.644343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.603453</td>\n",
       "      <td>0.698070</td>\n",
       "      <td>0.634781</td>\n",
       "      <td>0.395326</td>\n",
       "      <td>0.949005</td>\n",
       "      <td>0.957088</td>\n",
       "      <td>0.888962</td>\n",
       "      <td>0.962167</td>\n",
       "      <td>0.859328</td>\n",
       "      <td>0.859328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cos_ft  cos_ft_norm  cos_ft_norm_tfidf  cos_ft_tfidf  cos_nmf_c_v  \\\n",
       "0  0.776740     0.697593           0.715177      0.644239     0.521702   \n",
       "1  0.743313     0.772822           0.779357      0.000000     0.194543   \n",
       "2  0.668038     0.802900           0.812663      0.632182     0.007293   \n",
       "3  0.683037     0.595254           0.581431     -0.057154     0.511020   \n",
       "4  0.603453     0.698070           0.634781      0.395326     0.949005   \n",
       "\n",
       "   cos_nmf_tfidf  cos_svd_c_v  cos_svd_tfidf   cos_w2v  cos_w2v_tfidf  \n",
       "0       0.931057     0.137964       0.627718  0.849223       0.849223  \n",
       "1       0.920925     0.395494       0.738407  0.872423       0.872423  \n",
       "2       0.893944     0.094306       0.709270  0.928296       0.928296  \n",
       "3       0.897832     0.575513       0.689159  0.644343       0.644343  \n",
       "4       0.957088     0.888962       0.962167  0.859328       0.859328  "
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1-score: 0.5674339364884144'"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_result(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Перебор параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "SVD: [50, 100, 200]\n",
    "NMF: [50, 100, 200]\n",
    "w2v: size=[50, 100, 200]\n",
    "FastText: size=[50, 100, 200], max_n=[8, 15]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155b991d0adf4b2ba87df7ec1b78ac0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7cc3b6fff2486c99aa1951da6272e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_svd = []\n",
    "for x in tqdm([50, 100, 200]):\n",
    "    svd = TruncatedSVD(x)\n",
    "    svd.fit(X_c_v)\n",
    "    svd_c_v_1, svd_c_v_2, cos_svd_c_v = get_vectors(data, svd, c_v)\n",
    "    cos_svd.append(cos_svd_c_v)\n",
    "for x in tqdm([50, 100, 200]):\n",
    "    svd = TruncatedSVD(x)\n",
    "    svd.fit(X_tfidf_v)\n",
    "    svd_tfidf_v_1, svd_tfidf_v_2, cos_svd_tfidf = get_vectors(data, svd, tfidf_v)\n",
    "    cos_svd.append(cos_svd_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.5637384168849026\n",
      "F1-score: 0.5686350530143768\n",
      "F1-score: 0.5678713021371428\n"
     ]
    }
   ],
   "source": [
    "for each in cos_svd[:3]:\n",
    "    X['cos_svd_c_v'] = each\n",
    "    print(show_result(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лучше размерность 100\n",
    "X['cos_svd_c_v'] = cos_svd[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.560032037788467\n",
      "F1-score: 0.5749721513666372\n",
      "F1-score: 0.5698658885325582\n"
     ]
    }
   ],
   "source": [
    "for each in cos_svd[3:]:\n",
    "    X['cos_svd_tfidf'] = each\n",
    "    print(show_result(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лучше размерность 100\n",
    "X['cos_svd_tfidf'] = cos_svd[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc8faf40d9e46c5aef6ee2dbc897941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8beef3e6960e4dcf89f5856f3e6d9867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_nmf = []\n",
    "for x in tqdm([50, 100, 200]):\n",
    "    nmf = NMF(x)\n",
    "    nmf.fit(X_c_v)\n",
    "    nmf_c_v_1, nmf_c_v_2, cos_nmf_c_v = get_vectors(data, nmf, c_v)\n",
    "    cos_nmf.append(cos_nmf_c_v)\n",
    "for x in tqdm([50, 100, 200]):\n",
    "    nmf = NMF(x)\n",
    "    nmf.fit(X_tfidf_v)\n",
    "    nmf_tfidf_v_1, nmf_tfidf_v_2, cos_nmf_tfidf = get_vectors(data, nmf, tfidf_v)\n",
    "    cos_nmf.append(cos_nmf_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.5665590124120888\n",
      "F1-score: 0.5686495605521474\n",
      "F1-score: 0.5703847337146141\n"
     ]
    }
   ],
   "source": [
    "for each in cos_svd[:3]:\n",
    "    X['cos_nmf_c_v'] = each\n",
    "    print(show_result(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лучше размерность 200\n",
    "X['cos_nmf_c_v'] = cos_nmf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.5778116437242705\n",
      "F1-score: 0.5776897594274245\n",
      "F1-score: 0.5775658317946109\n"
     ]
    }
   ],
   "source": [
    "for each in cos_svd[3:]:\n",
    "    X['cos_nmf_tfidf'] = each\n",
    "    print(show_result(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лучше размерность 50\n",
    "X['cos_nmf_tfidf'] = cos_nmf[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e48ebae6844a5c9f13523538f9472b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1cae4d35894a4a9d9316e26b9c27bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_w2v_l = []\n",
    "for x in tqdm([50, 100, 200]):\n",
    "    w2v = gensim.models.Word2Vec([text.split() for text in data_for_model['content_norm']], size=x, sg=1)\n",
    "    dim = x\n",
    "    X_text_1_w2v = np.zeros((len(data['text_1_norm'].values), dim))\n",
    "    X_text_2_w2v = np.zeros((len(data['text_2_norm'].values), dim))\n",
    "\n",
    "    for i, text in enumerate(data['text_1_norm'].values):\n",
    "        X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "    for i, text in enumerate(data['text_2_norm'].values):\n",
    "        X_text_2_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    cos_w2v = [float(cosine_similarity([x], [y])) for x, y in zip(X_text_1_w2v, X_text_2_w2v)]\n",
    "    cos_w2v_l.append(cos_w2v)\n",
    "for x in tqdm([50, 100, 200]):\n",
    "    w2v = gensim.models.Word2Vec([text.split() for text in data_for_model['content_norm']], size=x, sg=1)\n",
    "    dim = x\n",
    "    X_text_1_w2v = np.zeros((len(data['text_1_norm'].values), dim))\n",
    "    X_text_2_w2v = np.zeros((len(data['text_2_norm'].values), dim))\n",
    "\n",
    "    for i, text in enumerate(data['text_1_norm'].values):\n",
    "        X_text_1_w2v[i] = get_embedding(text, w2v, dim, tfidf=tf_idf_text_1[i])\n",
    "    \n",
    "    for i, text in enumerate(data['text_2_norm'].values):\n",
    "        X_text_2_w2v[i] = get_embedding(text, w2v, dim, tfidf=tf_idf_text_2[i])\n",
    "    cos_w2v = [float(cosine_similarity([x], [y])) for x, y in zip(X_text_1_w2v, X_text_2_w2v)]\n",
    "    cos_w2v_l.append(cos_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.5651272112522348\n",
      "F1-score: 0.5661895218715021\n",
      "F1-score: 0.5632773151419366\n"
     ]
    }
   ],
   "source": [
    "for each in cos_w2v_l[:3]:\n",
    "    X['cos_w2v'] = each\n",
    "    print(show_result(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лучше размерность 100\n",
    "X['cos_w2v'] = cos_w2v_l[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.5679348653121549\n",
      "F1-score: 0.5674843354379409\n",
      "F1-score: 0.5656971672415686\n"
     ]
    }
   ],
   "source": [
    "for each in cos_w2v_l[3:]:\n",
    "    X['cos_w2v_tfidf'] = each\n",
    "    print(show_result(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лучше размерность 50\n",
    "X['cos_w2v_tfidf'] = cos_svd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# берём лучший результат с FastText и параметры будем подбирать только тут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4219b6bdede74b289c9f22164808ae45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 8\n"
     ]
    }
   ],
   "source": [
    "cos_ft_l = []\n",
    "# с нормализацией, без tfidf\n",
    "for x in tqdm([50, 100, 200]):\n",
    "    for y in [8, 15]:\n",
    "        print(x, y)\n",
    "        fast_text = gensim.models.FastText([text.split() for text in data_for_model['content_norm']], size=x, min_n=4, max_n=y)\n",
    "        dim = x\n",
    "        X_text_1_ft = np.zeros((len(data['text_1_norm'].values), dim))\n",
    "        X_text_2_ft = np.zeros((len(data['text_2_norm'].values), dim))\n",
    "\n",
    "        for i, text in enumerate(data['text_1_norm'].values):\n",
    "            X_text_1_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "        for i, text in enumerate(data['text_2_norm'].values):\n",
    "            X_text_2_ft[i] = get_embedding(text, fast_text, dim)\n",
    "        cos_ft_tfidf = [float(cosine_similarity([x1], [y1])) for x1, y1 in zip(X_text_1_ft, X_text_2_ft)]\n",
    "        cos_ft_l.append(cos_ft_tfidf)\n",
    "        print(cos_ft_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(15) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-357-a62e024dfe7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcos_ft_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cos_ft_norm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-318-87232404b703>\u001b[0m in \u001b[0;36mshow_result\u001b[0;34m(X_text, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     clf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_leaf=10,\n\u001b[1;32m      4\u001b[0m                              class_weight='balanced')\n\u001b[1;32m      5\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 119\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(15) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "for each in cos_ft_l:\n",
    "    X['cos_ft_norm'] = each\n",
    "    print(show_result(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лучше размерность и max_n=\n",
    "X['cos_ft_norm'] = cos_ft_l[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_result(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
